{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11194a4a-24e4-4377-bf22-0f832d2bb522",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13223de5-0a4e-4856-a1ca-5c22347e1803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ─── Silence all outputs ──────────────────────────────────────────────\n",
    "import os, sys, warnings, logging\n",
    "\n",
    "# 1) No Python tracebacks\n",
    "sys.tracebacklimit = 0\n",
    "\n",
    "# 2) Drop all stderr (hides C/C++ loader logs too)\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "# 3) Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 4) Suppress all Python logging\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "698a8a3b-1cd4-4a7e-a803-26e29bf1feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymupdf\n",
    "# !pip install sentence-transformers\n",
    "# !pip install tf-keras\n",
    "# !pip install faiss-cpu\n",
    "# !pip install \"numpy<1.25\" --force-reinstall\n",
    "# !pip install pdfplumber transformers\n",
    "# !pip install \"transformers>=4.40\" accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f7111-a4a3-4727-8234-520e71698445",
   "metadata": {},
   "source": [
    "### Document Pre-processing & Chunk Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5bf3be7a-f93c-49e1-8448-b85e77cc8ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pathlib, pdfplumber\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "SKIP_PATTERNS = [\n",
    "    r\"^\\s*Samsung.*\", r\"^\\s*Warranty\\b\", r\"^\\s*Copyright\\b\",\n",
    "    r\"^\\s*Table of contents\\b\", r\"^\\s*\\d+\\s*$\"\n",
    "]\n",
    "_skip = re.compile(\"|\".join(SKIP_PATTERNS), re.I)\n",
    "\n",
    "def extract_pdf_text(pdf_path: str) -> List[str]:\n",
    "    lines = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for p in pdf.pages:\n",
    "            txt = p.extract_text() or \"\"\n",
    "            lines.extend(txt.splitlines())\n",
    "    return lines\n",
    "\n",
    "def clean_lines(raw_lines: List[str]) -> List[str]:\n",
    "    return [ln.strip() for ln in raw_lines\n",
    "            if ln.strip() and not _skip.match(ln)]\n",
    "\n",
    "def chunk_with_overlap(text_lines: List[str], tokenizer,\n",
    "                       max_tokens: int = 350, overlap: int = 50):\n",
    "    chunks, cur, cur_len = [], [], 0\n",
    "    for ln in text_lines:\n",
    "        tok = tokenizer.encode(ln, add_special_tokens=False)\n",
    "        if cur_len + len(tok) > max_tokens and cur:\n",
    "            chunks.append(cur)\n",
    "            cur = cur[-overlap:] if overlap else []\n",
    "            cur_len = len(cur)\n",
    "        cur.extend(tok);  cur_len += len(tok)\n",
    "    if cur: chunks.append(cur)\n",
    "    return [tokenizer.decode(toks) for toks in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "affa7133-4783-4a34-81fb-67d9c4a0c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(chunks)=249  (예시) user guide sm - s921w sm - s926w sm - s928w english ( ca ). 01 / 2024. rev. 1. 0 www. samsung. com / ca getting started 95 multi window ( using multiple apps at once ) 5 device layout and functions 98...\n"
     ]
    }
   ],
   "source": [
    "pdf_file   = \"yourpath/data.pdf\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "raw  = extract_pdf_text(pdf_file)\n",
    "clean = clean_lines(raw)\n",
    "chunks = chunk_with_overlap(clean, tokenizer,\n",
    "                            max_tokens=256, overlap=32)\n",
    "\n",
    "print(f\"{len(chunks)=}  (예시) {chunks[0][:200]}...\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = model.encode(chunks, show_progress_bar=True,\n",
    "                          normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d89e9f0-1a45-4226-a6cb-10781a52fb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 249 chunks indexed\n"
     ]
    }
   ],
   "source": [
    "import faiss, numpy as np, pickle, pathlib\n",
    "\n",
    "vecs = np.asarray(embeddings, dtype=\"float32\")          # (n_chunks, dim)\n",
    "dim  = vecs.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatIP(dim)                          # inner-product == cosine\n",
    "index.add(vecs)                                         # insert all chunk vectors\n",
    "\n",
    "faiss.write_index(index, \"samsung_manual.index\")\n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)\n",
    "\n",
    "print(f\" {index.ntotal} chunks indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7db715d9-f0f1-4e3f-bb52-2abc2d96ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_manual(query: str, k: int = 5, score_thres: float = 0.30):\n",
    "    q_vec = model.encode([query], normalize_embeddings=True)\n",
    "    D, I  = index.search(np.asarray(q_vec, dtype=\"float32\"), k)\n",
    "    hits  = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if score < score_thres:     \n",
    "            continue\n",
    "        hits.append({\"chunk_id\": int(idx),\n",
    "                     \"score\": float(score),\n",
    "                     \"text\":  chunks[idx]})\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aaa0cf17-81f4-47a9-9503-ad8cb50716c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in ./env/lib/python3.11/site-packages (0.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./env/lib/python3.11/site-packages (from llama-cpp-python) (4.14.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./env/lib/python3.11/site-packages (from llama-cpp-python) (1.24.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in ./env/lib/python3.11/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in ./env/lib/python3.11/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys, importlib\n",
    "\n",
    "def pip_install(pkg):\n",
    "    if importlib.util.find_spec(pkg) is None:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "pip_install(\"huggingface_hub\")     \n",
    "pip_install(\"llama-cpp-python\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ce908-3237-4f98-9f1e-2e02414d3625",
   "metadata": {},
   "source": [
    "### TinyLlama download & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41858691-2e96-4d28-b96e-f4fd2b09b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "repo_id   = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
    "file_name = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"    # 4-bit, super-light\n",
    "local_path = hf_hub_download(repo_id, file_name)\n",
    "\n",
    "llm = Llama(model_path=local_path,\n",
    "            n_ctx=2048,         \n",
    "            n_threads=8,\n",
    "            n_batch=512)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "866d61d5-199f-4045-bc84-ab250fd65a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ all artifacts saved to /Users/namyoon/Desktop/UMN_MSBA/2025 Summer/NLP/Assignment2\n"
     ]
    }
   ],
   "source": [
    "import pathlib, pickle, numpy as np, faiss, os\n",
    "\n",
    "OUT = pathlib.Path(\"path\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(OUT / \"cleaned_paragraphs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)               # ← chunks = list[str]\n",
    "\n",
    "np.save(OUT / \"embedding_matrix.npy\", embeddings)   # embeddings = np.array\n",
    "\n",
    "faiss.write_index(index, str(OUT / \"faiss_index.faiss\"))\n",
    "print(\"✓ all artifacts saved to\", OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a69abf30-62ea-47b0-b424-db20431a4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, faiss, pathlib\n",
    "DATA = pathlib.Path(\"path\")\n",
    "texts = pickle.load(open(DATA / \"cleaned_paragraphs.pkl\", \"rb\"))\n",
    "index = faiss.read_index(str(DATA / \"faiss_index.faiss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f766f-3862-42a5-8484-3027ebcc3ce1",
   "metadata": {},
   "source": [
    "### Embedder + retrieval fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5a31311-68cc-4420-a0a9-112f62aea1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")     \n",
    "\n",
    "def search_manual(query, k=3):\n",
    "    qv = embedder.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    D, I = index.search(qv, k)\n",
    "    return [{\"chunk_id\":int(i), \"text\":texts[i]} for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1b708-1cff-4d2c-add5-b0c5d89b8b79",
   "metadata": {},
   "source": [
    "### Prompt builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7af9e0ff-8d84-4f1d-9fbc-b02a1b05fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query: str, retrieved):\n",
    "    context = \"\\n\\n---\\n\\n\".join(\n",
    "        [f\"[#{h['chunk_id']}] {h['text']}\" for h in retrieved]\n",
    "    )\n",
    "    prompt = f\"\"\"You are a Samsung product-manual assistant.\n",
    "Use ONLY the context to answer. If not answerable, say you don't know.\n",
    "Show the chunk id you used at the end.\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\n",
    "### Question\n",
    "{query}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eab170-9ca9-4011-97f6-2662c6534fdd",
   "metadata": {},
   "source": [
    "### LLM ask fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd74c3e0-39d7-4447-a569-73d08bc41624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_local_llm(prompt: str, max_tokens: int = 128) -> str:\n",
    "    out = llm(prompt, max_tokens=max_tokens, stop=[\"### Question\"])\n",
    "    return out[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d58c84-719e-41c0-9e8d-50d83d567019",
   "metadata": {},
   "source": [
    "### Test / Streamlit hand-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f08f4b1-d2ed-4120-8ffe-670c1c759f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Answer:\n",
      " To enable power saving mode, follow these steps:\n",
      "\n",
      "1. On the Settings screen, tap Battery.\n",
      "2. Scroll down to Power Saving and tap Enable Power Saving.\n",
      "3. Turn on Power Saving Mode to save battery power.\n"
     ]
    }
   ],
   "source": [
    "query   = \"How do I enable power saving mode?\"\n",
    "hits    = search_manual(query, k=3) \n",
    "prompt  = build_prompt(query, hits)\n",
    "\n",
    "answer  = ask_local_llm(prompt)\n",
    "print(\"\\n🔹 Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae52f6d-79ff-41b4-99c4-38598d80caad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-RAG (env)",
   "language": "python",
   "name": "nlp-rag-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
